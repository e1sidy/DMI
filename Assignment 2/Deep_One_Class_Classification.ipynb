{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_One_Class_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOvpttr5xGwh"
      },
      "source": [
        "# **DEEP ONE CLASS CLASSIFICATION**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xusp5TJxqk4",
        "outputId": "a5563f60-dab0-4b7b-93d4-1f4a0684421e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EFppJHYs6iF"
      },
      "source": [
        "# Importing Necessary File\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from barbar import Bar\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhuQWYIvoqXE"
      },
      "source": [
        "def normalWeightInitialization(para):\n",
        "    className = para.__class__.__name__\n",
        "    if className.find(\"Conv\") != -1 and className != 'Conv':\n",
        "        torch.nn.init.normal_(para.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"Linear\") != -1:\n",
        "        torch.nn.init.normal_(para.weight.data, 0.0, 0.02)\n",
        "\n",
        "\n",
        "def globalContrastNormalization(para):\n",
        "    #Apply global contrast normalization to tensor\n",
        "    avg = torch.mean(para)  # mean over all features (pixels) per sample\n",
        "    para -= avg\n",
        "    paraScale = torch.mean(torch.abs(para))\n",
        "    para /= paraScale\n",
        "    return para"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JeFW8yq7lKp"
      },
      "source": [
        "class MNISTLoader(data.Dataset):\n",
        "    #This class is needed to processing batches for the dataloader.\n",
        "    def __init__(self, data, target, transform):\n",
        "        self.data = data\n",
        "        self.target = target\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"return transformed items.\"\"\"\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        if self.transform:\n",
        "            x = Image.fromarray(x.numpy(), mode='L')\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"number of samples.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def get_mnist(args, data_dir='/content/drive/MyDrive/Data'):\n",
        "    # get dataloders\n",
        "    # min, max values for each class after applying GCN (as the original implementation)\n",
        "    miniAndmaxi = [(-0.8826567065619495, 9.001545489292527),\n",
        "                (-0.6661464580883915, 20.108062262467364),\n",
        "                (-0.7820454743183202, 11.665100841080346),\n",
        "                (-0.7645772083211267, 12.895051191467457),\n",
        "                (-0.7253923114302238, 12.683235701611533),\n",
        "                (-0.7698501867861425, 13.103278415430502),\n",
        "                (-0.778418217980696, 10.457837397569108),\n",
        "                (-0.7129780970522351, 12.057777597673047),\n",
        "                (-0.8280402650205075, 10.581538445782988),\n",
        "                (-0.7369959242164307, 10.697039838804978)]\n",
        "\n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Lambda(lambda x: globalContrastNormalization(x)),\n",
        "                                    transforms.Normalize([miniAndmaxi[args.normal_class][0]],\n",
        "                                                         [miniAndmaxi[args.normal_class][1] \\\n",
        "                                                         -miniAndmaxi[args.normal_class][0]])])\n",
        "    train = datasets.MNIST(root=data_dir, train=True, download=True)\n",
        "    test = datasets.MNIST(root=data_dir, train=False, download=True)\n",
        "\n",
        "    trainingX = train.data\n",
        "    trainingY = train.targets\n",
        "\n",
        "    trainingX = trainingX[np.where(trainingY==args.normal_class)]\n",
        "    trainingY = trainingY[np.where(trainingY==args.normal_class)]\n",
        "                                    \n",
        "    trainingData = MNIST_loader(trainingX, trainingY, transform)\n",
        "    dataloaderTrain = DataLoader(trainingData, batch_size=args.batch_size, \n",
        "                                  shuffle=True, num_workers=0)\n",
        "    \n",
        "    testingX = test.data\n",
        "    testingY = test.targets\n",
        "    testingY = np.where(testingY==args.normal_class, 0, 1)\n",
        "    testingData = MNIST_loader(testingX, testingY, transform)\n",
        "    dataloaderTest = DataLoader(testingData, batch_size=args.batch_size, \n",
        "                                  shuffle=True, num_workers=0)\n",
        "    return dataloaderTrain, dataloaderTest"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QJePHsa6MuG",
        "outputId": "bbe0b428-ba90-4f9d-b1ac-16cb9d26945f"
      },
      "source": [
        "!pip install barbar"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: barbar in /usr/local/lib/python3.7/dist-packages (0.2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgKAIOuU6Yd3"
      },
      "source": [
        "class network(nn.Module):\n",
        "    def __init__(self, z_dim=32):\n",
        "        super(network, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "        self.fc1 = nn.Linear(4 * 7 * 7, z_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn1(x)))\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc1(x)\n",
        "\n",
        "\n",
        "class autoencoder(nn.Module):\n",
        "    def __init__(self, z_dim=32):\n",
        "        super(autoencoder, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "        self.fc1 = nn.Linear(4 * 7 * 7, z_dim, bias=False)\n",
        "\n",
        "        self.deconv1 = nn.ConvTranspose2d(2, 4, 5, bias=False, padding=2)\n",
        "        self.bn3 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "        self.deconv2 = nn.ConvTranspose2d(4, 8, 5, bias=False, padding=3)\n",
        "        self.bn4 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "        self.deconv3 = nn.ConvTranspose2d(8, 1, 5, bias=False, padding=2)\n",
        "        \n",
        "    def encode(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn1(x)))\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc1(x)\n",
        "   \n",
        "    def decode(self, x):\n",
        "        x = x.view(x.size(0), int(self.z_dim / 16), 4, 4)\n",
        "        x = F.interpolate(F.leaky_relu(x), scale_factor=2)\n",
        "        x = self.deconv1(x)\n",
        "        x = F.interpolate(F.leaky_relu(self.bn3(x)), scale_factor=2)\n",
        "        x = self.deconv2(x)\n",
        "        x = F.interpolate(F.leaky_relu(self.bn4(x)), scale_factor=2)\n",
        "        x = self.deconv3(x)\n",
        "        return torch.sigmoid(x)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        x_hat = self.decode(z)\n",
        "        return x_hat"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYGQPpOgrM8x"
      },
      "source": [
        "class TrainerDeepSVDD:\n",
        "    def __init__(self, args, data, device):\n",
        "        self.args = args\n",
        "        self.train_loader, self.test_loader = data\n",
        "        self.device = device\n",
        "    \n",
        "\n",
        "    def pretrain(self):\n",
        "        #Pretraining the weights for the deep SVDD network using autoencoder\n",
        "        ae = autoencoder(self.args.latent_dim).to(self.device)\n",
        "        ae.apply(weights_init_normal)\n",
        "        optimizer = optim.Adam(ae.parameters(), lr=self.args.lr_ae,\n",
        "                               weight_decay=self.args.weight_decay_ae)\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
        "                    milestones=self.args.lr_milestones, gamma=0.1)\n",
        "        \n",
        "        ae.train()\n",
        "        for epoch in range(self.args.num_epochs_ae):\n",
        "            total_loss = 0\n",
        "            for x, _ in Bar(self.train_loader):\n",
        "                x = x.float().to(self.device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                x_hat = ae(x)\n",
        "                reconst_loss = torch.mean(torch.sum((x_hat - x) ** 2, dim=tuple(range(1, x_hat.dim()))))\n",
        "                reconst_loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                total_loss += reconst_loss.item()\n",
        "            scheduler.step()\n",
        "            print('Pretraining Autoencoder... Epoch: {}, Loss: {:.3f}'.format(\n",
        "                   epoch, total_loss/len(self.train_loader)))\n",
        "        self.save_weights_for_DeepSVDD(ae, self.train_loader) \n",
        "    \n",
        "\n",
        "    def save_weights_for_DeepSVDD(self, model, dataloader):\n",
        "        #Initialize Deep SVDD weights using the encoder weights of the pretrained autoencoder.\n",
        "        c = self.set_c(model, dataloader)\n",
        "        net = network(self.args.latent_dim).to(self.device)\n",
        "        state_dict = model.state_dict()\n",
        "        net.load_state_dict(state_dict, strict=False)\n",
        "        torch.save({'center': c.cpu().data.numpy().tolist(),\n",
        "                    'net_dict': net.state_dict()}, '/content/drive/MyDrive/pretrained_parameters.pth')\n",
        "    \n",
        "\n",
        "    def set_c(self, model, dataloader, eps=0.1):\n",
        "        #Initializing the center for the hypersphere\n",
        "        model.eval()\n",
        "        z_ = []\n",
        "        with torch.no_grad():\n",
        "            for x, _ in dataloader:\n",
        "                x = x.float().to(self.device)\n",
        "                z = model.encode(x)\n",
        "                z_.append(z.detach())\n",
        "        z_ = torch.cat(z_)\n",
        "        c = torch.mean(z_, dim=0)\n",
        "        c[(abs(c) < eps) & (c < 0)] = -eps\n",
        "        c[(abs(c) < eps) & (c > 0)] = eps\n",
        "        return c\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        #Training the Deep SVDD model\n",
        "        net = network().to(self.device)\n",
        "        \n",
        "        if self.args.pretrain==True:\n",
        "            state_dict = torch.load('/content/drive/MyDrive/pretrained_parameters.pth')\n",
        "            net.load_state_dict(state_dict['net_dict'])\n",
        "            c = torch.Tensor(state_dict['center']).to(self.device)\n",
        "        else:\n",
        "            net.apply(weights_init_normal)\n",
        "            c = torch.randn(self.args.latent_dim).to(self.device)\n",
        "        \n",
        "        optimizer = optim.Adam(net.parameters(), lr=self.args.lr,\n",
        "                               weight_decay=self.args.weight_decay)\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
        "                    milestones=self.args.lr_milestones, gamma=0.1)\n",
        "\n",
        "        net.train()\n",
        "        for epoch in range(self.args.num_epochs):\n",
        "            total_loss = 0\n",
        "            for x, _ in Bar(self.train_loader):\n",
        "                x = x.float().to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                z = net(x)\n",
        "                loss = torch.mean(torch.sum((z - c) ** 2, dim=1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "            scheduler.step()\n",
        "            print('Training Deep SVDD... Epoch: {}, Loss: {:.3f}'.format(\n",
        "                   epoch, total_loss/len(self.train_loader)))\n",
        "        self.net = net\n",
        "        self.c = c"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CI9xn3Ls9vE"
      },
      "source": [
        "def eval(net, c, dataloader, device):\n",
        "    #Testing the Deep SVDD model\n",
        "\n",
        "    scores = []\n",
        "    labels = []\n",
        "    net.eval()\n",
        "    print('Testing...')\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.float().to(device)\n",
        "            z = net(x)\n",
        "            score = torch.sum((z - c) ** 2, dim=1)\n",
        "\n",
        "            scores.append(score.detach().cpu())\n",
        "            labels.append(y.cpu())\n",
        "    labels, scores = torch.cat(labels).numpy(), torch.cat(scores).numpy()\n",
        "    print('ROC AUC score: {:.2f}'.format(roc_auc_score(labels, scores)*100))\n",
        "    return labels, scores"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN_4Gy9964zr"
      },
      "source": [
        "class Args:\n",
        "  def __init__(self, nclass):\n",
        "    self.num_epochs=20\n",
        "    self.num_epochs_ae=20\n",
        "    self.patience=50\n",
        "    self.lr=1e-4\n",
        "    self.weight_decay=0.5e-6\n",
        "    self.weight_decay_ae=0.5e-3\n",
        "    self.lr_ae=1e-4\n",
        "    self.lr_milestones=[50]\n",
        "    self.batch_size=200\n",
        "    self.pretrain=True\n",
        "    self.latent_dim=32\n",
        "    self.normal_class=nclass"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxW94sFGEjju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d1991a5-db57-404d-defe-4bbcfac93a8d"
      },
      "source": [
        "nScores=[]\n",
        "inScores=[]\n",
        "outScores=[] \n",
        "for i in range(10):\n",
        "  args = Args(i)\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  data = get_mnist(args)\n",
        "  deep_SVDD = TrainerDeepSVDD(args, data, device)\n",
        "  if args.pretrain:\n",
        "      deep_SVDD.pretrain()\n",
        "  deep_SVDD.train()\n",
        "  print(\"test for calss  \",i,\".....\")\n",
        "  labels, scores = eval(deep_SVDD.net, deep_SVDD.c, data[1], device)\n",
        "  nScores.append(roc_auc_score(labels, scores)*100)\n",
        "  inScores.append(scores[np.where(labels==0)[0]])\n",
        "  outScores.append(scores[np.where(labels==1)[0]])\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5923/5923: [==============================>.] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 147.991\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 113.588\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 85.600\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 64.603\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 49.177\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 38.427\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 31.046\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 25.901\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 22.191\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 19.415\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 17.270\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 15.627\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 14.319\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 13.243\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 12.343\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 11.567\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 10.905\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 10.321\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 9.812\n",
            "5923/5923: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 9.365\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 0, Loss: 0.209\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 1, Loss: 0.073\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 2, Loss: 0.038\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 3, Loss: 0.026\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 4, Loss: 0.020\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 5, Loss: 0.017\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 6, Loss: 0.015\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 7, Loss: 0.013\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 8, Loss: 0.012\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 9, Loss: 0.010\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 10, Loss: 0.010\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 11, Loss: 0.009\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 12, Loss: 0.009\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 13, Loss: 0.008\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 14, Loss: 0.007\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 15, Loss: 0.007\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 16, Loss: 0.007\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 17, Loss: 0.006\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 18, Loss: 0.006\n",
            "5923/5923: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 19, Loss: 0.006\n",
            "test for calss   0 .....\n",
            "Testing...\n",
            "ROC AUC score: 97.40\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 171.058\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 127.801\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 93.660\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 68.789\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 51.322\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 39.465\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 31.437\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 25.738\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 21.578\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 18.476\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 16.081\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 14.195\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 12.679\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 11.437\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 10.402\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 9.537\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 8.799\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 8.162\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 7.607\n",
            "6742/6742: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 7.114\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 0, Loss: 0.074\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 1, Loss: 0.017\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 2, Loss: 0.010\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 3, Loss: 0.007\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 4, Loss: 0.006\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 5, Loss: 0.005\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 6, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 7, Loss: 0.004\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 8, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 9, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 10, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 11, Loss: 0.003\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 12, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 13, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 14, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 15, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 16, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 17, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 18, Loss: 0.002\n",
            "6742/6742: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 19, Loss: 0.002\n",
            "test for calss   1 .....\n",
            "Testing...\n",
            "ROC AUC score: 99.74\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 134.729\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 100.797\n",
            "5958/5958: [==============================>.] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 76.065\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 58.034\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 44.706\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 35.362\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 28.734\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 24.052\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 20.652\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 18.094\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 16.133\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 14.589\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 13.304\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 12.231\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 11.347\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 10.614\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 9.994\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 9.461\n",
            "5958/5958: [==============================>.] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 8.997\n",
            "5958/5958: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 8.589\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 0, Loss: 0.134\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 1, Loss: 0.058\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 2, Loss: 0.034\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 3, Loss: 0.024\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 4, Loss: 0.019\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 5, Loss: 0.015\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 6, Loss: 0.013\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 7, Loss: 0.011\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 8, Loss: 0.010\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 9, Loss: 0.009\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 10, Loss: 0.009\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 11, Loss: 0.008\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 12, Loss: 0.007\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 13, Loss: 0.007\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 14, Loss: 0.006\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 15, Loss: 0.007\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 16, Loss: 0.007\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 17, Loss: 0.006\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 18, Loss: 0.005\n",
            "5958/5958: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 19, Loss: 0.005\n",
            "test for calss   2 .....\n",
            "Testing...\n",
            "ROC AUC score: 86.78\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 156.134\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 120.763\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 88.340\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 64.360\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 48.239\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 37.411\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 30.001\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 24.801\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 21.030\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 18.210\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 16.054\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 14.371\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 13.029\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 11.930\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 11.011\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 10.234\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 9.574\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 9.006\n",
            "6131/6131: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 8.520\n",
            "6131/6131: [==============================>.] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 8.093\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 0, Loss: 0.129\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 1, Loss: 0.055\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 2, Loss: 0.036\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 3, Loss: 0.027\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 4, Loss: 0.022\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 5, Loss: 0.018\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 6, Loss: 0.016\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 7, Loss: 0.015\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 8, Loss: 0.013\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 9, Loss: 0.012\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 10, Loss: 0.011\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 11, Loss: 0.010\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 12, Loss: 0.010\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 13, Loss: 0.009\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 14, Loss: 0.008\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 15, Loss: 0.008\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 16, Loss: 0.008\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 17, Loss: 0.008\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 18, Loss: 0.007\n",
            "6131/6131: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 19, Loss: 0.007\n",
            "test for calss   3 .....\n",
            "Testing...\n",
            "ROC AUC score: 90.91\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 151.991\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 114.841\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 85.410\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 63.895\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 49.025\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 38.956\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 31.947\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 26.878\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 23.118\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 20.282\n",
            "5842/5842: [==============================>.] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 18.064\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 16.307\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 14.876\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 13.726\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 12.745\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 11.926\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 11.220\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 10.597\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 10.061\n",
            "5842/5842: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 9.565\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 0, Loss: 0.131\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 1, Loss: 0.058\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 2, Loss: 0.038\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 3, Loss: 0.029\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 4, Loss: 0.024\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 5, Loss: 0.020\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 6, Loss: 0.017\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 7, Loss: 0.016\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 8, Loss: 0.013\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 9, Loss: 0.013\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 10, Loss: 0.011\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 11, Loss: 0.010\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 12, Loss: 0.010\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 13, Loss: 0.009\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 14, Loss: 0.009\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 15, Loss: 0.008\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 16, Loss: 0.008\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 17, Loss: 0.008\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 18, Loss: 0.007\n",
            "5842/5842: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 19, Loss: 0.007\n",
            "test for calss   4 .....\n",
            "Testing...\n",
            "ROC AUC score: 96.20\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 162.977\n",
            "5421/5421: [==============================>.] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 126.668\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 96.948\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 73.828\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 57.252\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 45.465\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 36.971\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 30.845\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 26.301\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 22.823\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 20.155\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 18.037\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 16.382\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 14.968\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 13.820\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 12.860\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 12.049\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 11.310\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 10.729\n",
            "5421/5421: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 10.178\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 0, Loss: 0.256\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 1, Loss: 0.115\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 2, Loss: 0.068\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 3, Loss: 0.046\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 4, Loss: 0.037\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 5, Loss: 0.030\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 6, Loss: 0.025\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 7, Loss: 0.022\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 8, Loss: 0.020\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 9, Loss: 0.019\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 10, Loss: 0.017\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 11, Loss: 0.015\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 12, Loss: 0.014\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 13, Loss: 0.014\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 14, Loss: 0.012\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 15, Loss: 0.013\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 16, Loss: 0.012\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 17, Loss: 0.010\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 18, Loss: 0.010\n",
            "5421/5421: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 19, Loss: 0.010\n",
            "test for calss   5 .....\n",
            "Testing...\n",
            "ROC AUC score: 83.77\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 140.012\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 105.789\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 78.859\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 59.034\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 45.343\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 36.061\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 29.664\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 25.082\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 21.676\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 19.116\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 17.137\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 15.584\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 14.315\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 13.268\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 12.378\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 11.607\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 10.939\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 10.360\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 9.846\n",
            "5918/5918: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 9.397\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 0, Loss: 0.208\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 1, Loss: 0.094\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 2, Loss: 0.055\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 3, Loss: 0.039\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 4, Loss: 0.030\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 5, Loss: 0.025\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 6, Loss: 0.021\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 7, Loss: 0.018\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 8, Loss: 0.016\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 9, Loss: 0.015\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 10, Loss: 0.013\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 11, Loss: 0.012\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 12, Loss: 0.011\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 13, Loss: 0.010\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 14, Loss: 0.011\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 15, Loss: 0.009\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 16, Loss: 0.009\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 17, Loss: 0.009\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 18, Loss: 0.008\n",
            "5918/5918: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 19, Loss: 0.008\n",
            "test for calss   6 .....\n",
            "Testing...\n",
            "ROC AUC score: 97.98\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 146.739\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 109.785\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 80.156\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 59.631\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 45.884\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 36.365\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 29.784\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 25.057\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 21.531\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 18.889\n",
            "6265/6265: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 16.853\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 15.259\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 13.975\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 12.912\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 12.026\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 11.271\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 10.622\n",
            "6265/6265: [===============================>] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 10.051\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 9.546\n",
            "6265/6265: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 9.080\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 0, Loss: 0.322\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 1, Loss: 0.114\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 2, Loss: 0.061\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 3, Loss: 0.041\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 4, Loss: 0.031\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 5, Loss: 0.025\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 6, Loss: 0.020\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 7, Loss: 0.017\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 8, Loss: 0.015\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 9, Loss: 0.014\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 10, Loss: 0.012\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 11, Loss: 0.011\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 12, Loss: 0.010\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 13, Loss: 0.010\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 14, Loss: 0.009\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 15, Loss: 0.008\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 16, Loss: 0.008\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 17, Loss: 0.008\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 18, Loss: 0.007\n",
            "6265/6265: [===============================>] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 19, Loss: 0.007\n",
            "test for calss   7 .....\n",
            "Testing...\n",
            "ROC AUC score: 94.83\n",
            "5851/5851: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 139.505\n",
            "5851/5851: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 105.773\n",
            "5851/5851: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 78.819\n",
            "5851/5851: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 59.160\n",
            "5851/5851: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 45.234\n",
            "5851/5851: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 35.674\n",
            "5851/5851: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 29.011\n",
            "5851/5851: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 24.271\n",
            "5851/5851: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 20.808\n",
            "5851/5851: [==============================>.] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 18.211\n",
            "5851/5851: [==============================>.] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 16.223\n",
            "5851/5851: [==============================>.] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 14.643\n",
            "5851/5851: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 13.381\n",
            "5851/5851: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 12.330\n",
            "5851/5851: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 11.459\n",
            "5851/5851: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 10.741\n",
            "5851/5851: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 10.150\n",
            "5851/5851: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 9.614\n",
            "5851/5851: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 9.174\n",
            "5851/5851: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 8.785\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 0, Loss: 0.155\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 1, Loss: 0.060\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 2, Loss: 0.036\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 3, Loss: 0.027\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 4, Loss: 0.022\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 5, Loss: 0.018\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 6, Loss: 0.015\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 7, Loss: 0.013\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 8, Loss: 0.012\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 9, Loss: 0.010\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 10, Loss: 0.010\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 11, Loss: 0.009\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 12, Loss: 0.008\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 13, Loss: 0.008\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 14, Loss: 0.007\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 15, Loss: 0.006\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 16, Loss: 0.006\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 17, Loss: 0.006\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 18, Loss: 0.006\n",
            "5851/5851: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 19, Loss: 0.006\n",
            "test for calss   8 .....\n",
            "Testing...\n",
            "ROC AUC score: 93.17\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 143.950\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 107.809\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 79.630\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 60.163\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 46.770\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 37.571\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 31.135\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 26.494\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 23.014\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 20.342\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 18.263\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 16.631\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 15.313\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 14.236\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 13.327\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 12.563\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 11.894\n",
            "5949/5949: [==============================>.] - ETA 0.3s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 11.308\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 10.796\n",
            "5949/5949: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 10.343\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 0, Loss: 0.157\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 1, Loss: 0.067\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 2, Loss: 0.039\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 3, Loss: 0.028\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 4, Loss: 0.021\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 5, Loss: 0.018\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 6, Loss: 0.015\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 7, Loss: 0.013\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 8, Loss: 0.011\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 9, Loss: 0.010\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 10, Loss: 0.009\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 11, Loss: 0.008\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 12, Loss: 0.008\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 13, Loss: 0.008\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 14, Loss: 0.007\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 15, Loss: 0.007\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 16, Loss: 0.006\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 17, Loss: 0.006\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 18, Loss: 0.006\n",
            "5949/5949: [==============================>.] - ETA 0.1s\n",
            "Training Deep SVDD... Epoch: 19, Loss: 0.005\n",
            "test for calss   9 .....\n",
            "Testing...\n",
            "ROC AUC score: 96.90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OduNXNF5t_a8",
        "outputId": "f90f733d-e953-4a83-d8e7-2517c0a6dc87"
      },
      "source": [
        "for i in range(10):\n",
        "  print(\"ROC scores for class \",i,\" is: \",nScores[i])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROC scores for class  0  is:  97.40487126114303\n",
            "ROC scores for class  1  is:  99.73731275048388\n",
            "ROC scores for class  2  is:  86.77848543313348\n",
            "ROC scores for class  3  is:  90.91221268956706\n",
            "ROC scores for class  4  is:  96.20082080690395\n",
            "ROC scores for class  5  is:  83.77114142004959\n",
            "ROC scores for class  6  is:  97.98172204036003\n",
            "ROC scores for class  7  is:  94.82890783431723\n",
            "ROC scores for class  8  is:  93.17472544522303\n",
            "ROC scores for class  9  is:  96.90018175867752\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "bmW-HjT3t2jB",
        "outputId": "c4889421-612f-4490-bfba-e98693d6d852"
      },
      "source": [
        "\"\"\"\n",
        "for i in range(10):\n",
        "  in_ = pd.DataFrame(inScores[i], columns=['Inlier'])\n",
        "  out_ = pd.DataFrame(outScores[i], columns=['Outlier'])\n",
        "  fig, ax = plt.subplots()\n",
        "  in_.plot.kde(ax=ax, legend=True, title='Outliers vs Inliers for class '+str(i))\n",
        "  out_.plot.kde(ax=ax, legend=True)\n",
        "  plt.xlim(-0.05, 0.08)\n",
        "  ax.grid(axis='x')\n",
        "  ax.grid(axis='y')\n",
        "  plt.show()\n",
        "\"\"\""
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nfor i in range(10):\\n  in_ = pd.DataFrame(inScores[i], columns=['Inlier'])\\n  out_ = pd.DataFrame(outScores[i], columns=['Outlier'])\\n  fig, ax = plt.subplots()\\n  in_.plot.kde(ax=ax, legend=True, title='Outliers vs Inliers for class '+str(i))\\n  out_.plot.kde(ax=ax, legend=True)\\n  plt.xlim(-0.05, 0.08)\\n  ax.grid(axis='x')\\n  ax.grid(axis='y')\\n  plt.show()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    }
  ]
}