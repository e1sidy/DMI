{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SS_DSVDD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bHXhDHc8pIo"
      },
      "source": [
        "#SS_DSVDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOv397sBDt_v"
      },
      "source": [
        "#IMPORTED FILES AND NECESSARY MODULES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuSPS4f4AqNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c90a0a-1cdc-4e81-f695-f4016dbfac26"
      },
      "source": [
        "!pip install barbar\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from barbar import Bar\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: barbar in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5nlRlVdETiR"
      },
      "source": [
        "# UTILITY CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L60vKeoafA3e"
      },
      "source": [
        "\n",
        "class MyMNIST(MNIST):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(MyMNIST, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.semi_targets = torch.zeros_like(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target, semi_target = self.data[index], int(self.targets[index]), int(self.semi_targets[index])\n",
        "        img = Image.fromarray(img.numpy(), mode='L')\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target, semi_target, index\n",
        "class network(nn.Module):\n",
        "    def __init__(self, z_dim=32):\n",
        "        super(network, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "        self.fc1 = nn.Linear(4 * 7 * 7, z_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn1(x)))\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc1(x)\n",
        "\n",
        "\n",
        "class encoder(nn.Module):\n",
        "    def __init__(self, z_dim=32):\n",
        "        super(autoencoder, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 8, 5, bias=False, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "        self.conv2 = nn.Conv2d(8, 4, 5, bias=False, padding=2)\n",
        "        self.bn2 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "        self.fc1 = nn.Linear(4 * 7 * 7, z_dim, bias=False)\n",
        "\n",
        "        self.deconv1 = nn.ConvTranspose2d(2, 4, 5, bias=False, padding=2)\n",
        "        self.bn3 = nn.BatchNorm2d(4, eps=1e-04, affine=False)\n",
        "        self.deconv2 = nn.ConvTranspose2d(4, 8, 5, bias=False, padding=3)\n",
        "        self.bn4 = nn.BatchNorm2d(8, eps=1e-04, affine=False)\n",
        "        self.deconv3 = nn.ConvTranspose2d(8, 1, 5, bias=False, padding=2)\n",
        "        \n",
        "    def encode(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn1(x)))\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool(F.leaky_relu(self.bn2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc1(x)\n",
        "   \n",
        "    def decode(self, x):\n",
        "        x = x.view(x.size(0), int(self.z_dim / 16), 4, 4)\n",
        "        x = F.interpolate(F.leaky_relu(x), scale_factor=2)\n",
        "        x = self.deconv1(x)\n",
        "        x = F.interpolate(F.leaky_relu(self.bn3(x)), scale_factor=2)\n",
        "        x = self.deconv2(x)\n",
        "        x = F.interpolate(F.leaky_relu(self.bn4(x)), scale_factor=2)\n",
        "        x = self.deconv3(x)\n",
        "        return torch.sigmoid(x)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        x_hat = self.decode(z)\n",
        "        return x_hat\n",
        "class TrainerDeepSAD:\n",
        "    def __init__(self, args, data, device):\n",
        "        self.args = args\n",
        "        self.train_loader, self.test_loader = data\n",
        "        self.device = device\n",
        "    \n",
        "\n",
        "    def pretrain(self):\n",
        "        ae = autoencoder(self.args.latent_dim).to(self.device)\n",
        "        ae.apply(weights_init_normal)\n",
        "        optimizer = optim.Adam(ae.parameters(), lr=self.args.lr_ae,\n",
        "                               weight_decay=self.args.weight_decay_ae)\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
        "                    milestones=self.args.lr_milestones, gamma=0.1)\n",
        "        \n",
        "        ae.train()\n",
        "        for epoch in range(self.args.num_epochs_ae):\n",
        "            total_loss = 0\n",
        "            for x, _, _, _ in Bar(self.train_loader):\n",
        "                x = x.float().to(self.device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                x_hat = ae(x)\n",
        "                reconst_loss = torch.mean(torch.sum((x_hat - x) ** 2, dim=tuple(range(1, x_hat.dim()))))\n",
        "                reconst_loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                total_loss += reconst_loss.item()\n",
        "            scheduler.step()\n",
        "            print('Pretraining Autoencoder... Epoch: {}, Loss: {:.3f}'.format(\n",
        "                   epoch, total_loss/len(self.train_loader)))\n",
        "        self.save_weights_for_DeepSAD(ae, self.train_loader) \n",
        "    \n",
        "\n",
        "    def save_weights_for_DeepSAD(self, model, dataloader):\n",
        "        c = self.set_c(model, dataloader)\n",
        "        net = network(self.args.latent_dim).to(self.device)\n",
        "        state_dict = model.state_dict()\n",
        "        net.load_state_dict(state_dict, strict=False)\n",
        "        torch.save({'center': c.cpu().data.numpy().tolist(),\n",
        "                    'net_dict': net.state_dict()}, '/content/drive/MyDrive/pretrained_parameters.pth')\n",
        "    \n",
        "\n",
        "    def set_c(self, model, dataloader, eps=0.1):\n",
        "        model.eval()\n",
        "        z_ = []\n",
        "        with torch.no_grad():\n",
        "            for x, _, _, _ in dataloader:\n",
        "                x = x.float().to(self.device)\n",
        "                z = model.encode(x)\n",
        "                z_.append(z.detach())\n",
        "        z_ = torch.cat(z_)\n",
        "        c = torch.mean(z_, dim=0)\n",
        "        c[(abs(c) < eps) & (c < 0)] = -eps\n",
        "        c[(abs(c) < eps) & (c > 0)] = eps\n",
        "        return c\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        net = network().to(self.device)\n",
        "        eta=1\n",
        "        eps=1e-6\n",
        "        if self.args.pretrain==True:\n",
        "            state_dict = torch.load('/content/drive/MyDrive/pretrained_parameters.pth')\n",
        "            net.load_state_dict(state_dict['net_dict'])\n",
        "            c = torch.Tensor(state_dict['center']).to(self.device)\n",
        "        else:\n",
        "            net.apply(weights_init_normal)\n",
        "            c = torch.randn(self.args.latent_dim).to(self.device)\n",
        "        \n",
        "        optimizer = optim.Adam(net.parameters(), lr=self.args.lr,\n",
        "                               weight_decay=self.args.weight_decay)\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
        "                    milestones=self.args.lr_milestones, gamma=0.1)\n",
        "\n",
        "        net.train()\n",
        "        for epoch in range(self.args.num_epochs):\n",
        "            total_loss = 0\n",
        "            for x, _, sy, _ in Bar(self.train_loader):\n",
        "                x = x.float().to(self.device)\n",
        "                sy=sy.float().to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                z = net(x)\n",
        "                dist = torch.sum((z-c) ** 2, dim=1)\n",
        "                losses = torch.where(sy == 0, dist, eta * ((dist + eps) ** sy.float()))\n",
        "                loss = torch.mean(losses)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "            scheduler.step()\n",
        "            print('Training Deep SAD... Epoch: {}, Loss: {:.3f}'.format(\n",
        "                   epoch, total_loss/len(self.train_loader)))\n",
        "        self.net = net\n",
        "        self.c = c\n",
        "class Args:\n",
        "  def __init__(self, nclass):\n",
        "    self.num_epochs=20\n",
        "    self.num_epochs_ae=20\n",
        "    self.patience=50\n",
        "    self.lr=1e-4\n",
        "    self.weight_decay=0.5e-6\n",
        "    self.weight_decay_ae=0.5e-3\n",
        "    self.lr_ae=1e-4\n",
        "    self.lr_milestones=[50]\n",
        "    self.batch_size=200\n",
        "    self.pretrain=True\n",
        "    self.latent_dim=32\n",
        "    self.normal_class=nclass\n",
        "    self.ratio_known_outlier=0.01\n",
        "    self.ratio_known_normal=0.0\n",
        "    self.ratio_pollution=0.1\n",
        "    self.known_outlier_class=(nclass+1)%10\n",
        "    self.n_known_outlier_classes=1"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXpbqPrAElOD"
      },
      "source": [
        "# UTILITY FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S7IgWq6Gx-c"
      },
      "source": [
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1 and classname != 'Conv':\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"Linear\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "def create_semisupervised_setting(labels, normal_classes, outlier_classes, known_outlier_classes,ratio_known_normal, ratio_known_outlier, ratio_pollution):\n",
        "    idx_normal = np.argwhere(np.isin(labels, normal_classes)).flatten()\n",
        "    idx_outlier = np.argwhere(np.isin(labels, outlier_classes)).flatten()\n",
        "    idx_known_outlier_candidates = np.argwhere(np.isin(labels, known_outlier_classes)).flatten()\n",
        "\n",
        "    n_normal = len(idx_normal)\n",
        "\n",
        "    a = np.array([[1, 1, 0, 0],\n",
        "                  [(1-ratio_known_normal), -ratio_known_normal, -ratio_known_normal, -ratio_known_normal],\n",
        "                  [-ratio_known_outlier, -ratio_known_outlier, -ratio_known_outlier, (1-ratio_known_outlier)],\n",
        "                  [0, -ratio_pollution, (1-ratio_pollution), 0]])\n",
        "    b = np.array([n_normal, 0, 0, 0])\n",
        "    x = np.linalg.solve(a, b)\n",
        "\n",
        "    n_known_normal = int(x[0])\n",
        "    n_unlabeled_normal = int(x[1])\n",
        "    n_unlabeled_outlier = int(x[2])\n",
        "    n_known_outlier = int(x[3])\n",
        "\n",
        "    perm_normal = np.random.permutation(n_normal)\n",
        "    perm_outlier = np.random.permutation(len(idx_outlier))\n",
        "    perm_known_outlier = np.random.permutation(len(idx_known_outlier_candidates))\n",
        "\n",
        "    idx_known_normal = idx_normal[perm_normal[:n_known_normal]].tolist()\n",
        "    idx_unlabeled_normal = idx_normal[perm_normal[n_known_normal:n_known_normal+n_unlabeled_normal]].tolist()\n",
        "    idx_unlabeled_outlier = idx_outlier[perm_outlier[:n_unlabeled_outlier]].tolist()\n",
        "    idx_known_outlier = idx_known_outlier_candidates[perm_known_outlier[:n_known_outlier]].tolist()\n",
        "\n",
        "    labels_known_normal = labels[idx_known_normal].tolist()\n",
        "    labels_unlabeled_normal = labels[idx_unlabeled_normal].tolist()\n",
        "    labels_unlabeled_outlier = labels[idx_unlabeled_outlier].tolist()\n",
        "    labels_known_outlier = labels[idx_known_outlier].tolist()\n",
        "\n",
        "    semi_labels_known_normal = np.ones(n_known_normal).astype(np.int32).tolist()\n",
        "    semi_labels_unlabeled_normal = np.zeros(n_unlabeled_normal).astype(np.int32).tolist()\n",
        "    semi_labels_unlabeled_outlier = np.zeros(n_unlabeled_outlier).astype(np.int32).tolist()\n",
        "    semi_labels_known_outlier = (-np.ones(n_known_outlier).astype(np.int32)).tolist()\n",
        "\n",
        "    list_idx = idx_known_normal + idx_unlabeled_normal + idx_unlabeled_outlier + idx_known_outlier\n",
        "    list_labels = labels_known_normal + labels_unlabeled_normal + labels_unlabeled_outlier + labels_known_outlier\n",
        "    list_semi_labels = (semi_labels_known_normal + semi_labels_unlabeled_normal + semi_labels_unlabeled_outlier\n",
        "                        + semi_labels_known_outlier)\n",
        "\n",
        "    return list_idx, list_labels, list_semi_labels\n",
        "def get_mnist(args, data_dir='/content/drive/MyDrive/Data'):\n",
        "    n_classes = 2\n",
        "    normal_classes = tuple([args.normal_class])\n",
        "    outlier_classes = list(range(0, 10))\n",
        "    outlier_classes.remove(args.normal_class)\n",
        "    outlier_classes = tuple(outlier_classes)\n",
        "\n",
        "    if args.n_known_outlier_classes == 0:\n",
        "        known_outlier_classes = ()\n",
        "    elif args.n_known_outlier_classes == 1:\n",
        "        known_outlier_classes = tuple([args.known_outlier_class])\n",
        "    else:\n",
        "        known_outlier_classes = tuple(random.sample(outlier_classes, args.n_known_outlier_classes))\n",
        "    \n",
        "    transform = transforms.ToTensor()\n",
        "    target_transform = transforms.Lambda(lambda x: int(x in outlier_classes))\n",
        "    \n",
        "    train = DataSet(root=data_dir, train=True,transform=transform, target_transform=target_transform,download=True)\n",
        "    idx, _, semi_targets = create_semisupervised_setting(train.targets.cpu().data.numpy(), normal_classes,\n",
        "                                                             outlier_classes, known_outlier_classes,\n",
        "                                                             args.ratio_known_normal, args.ratio_known_outlier, args.ratio_pollution)\n",
        "    train.semi_targets[idx] = torch.tensor(semi_targets)\n",
        "    train = Subset(train, idx)          \n",
        "    dataloader_train = DataLoader(train, batch_size=args.batch_size, \n",
        "                                  shuffle=True, num_workers=0)\n",
        "    \n",
        "    test = DataSet(root=data_dir, train=False, transform=transform, target_transform=target_transform,\n",
        "                                download=True)\n",
        "    dataloader_test = DataLoader(test, batch_size=args.batch_size, \n",
        "                                  shuffle=True, num_workers=0)\n",
        "    return dataloader_train, dataloader_test"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6F0bnW_EtNF"
      },
      "source": [
        "# OUTPUT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxW94sFGEjju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aa3fc61-5b22-4bf0-8519-d074428d4700"
      },
      "source": [
        "nscores=[]\n",
        "roc=[]\n",
        "mean=[]\n",
        "for i in range(10):\n",
        "  args = Args(i)\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  data = get_mnist(args)\n",
        "  deep_SAD = TrainerDeepSAD(args, data, device)\n",
        "  if args.pretrain:\n",
        "      deep_SAD.pretrain()\n",
        "  deep_SAD.train()\n",
        "  print(\"test for calss  \",i,\".....\")\n",
        "  net=deep_SAD.net\n",
        "  c=deep_SAD.c\n",
        "  dataloader=data[1]\n",
        "  nscores.append(roc_auc_score(labels, scores)*100)\n",
        "  scores = []\n",
        "  labels = []\n",
        "  epoch_loss=0\n",
        "  nb=0\n",
        "  eta=1.0\n",
        "  eps=1e-6\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "      for x, y, sy,_ in dataloader:\n",
        "          x = x.float().to(device)\n",
        "          y = y.float().to(device)\n",
        "          sy= sy.float().to(device)\n",
        "          z = net(x)\n",
        "          dist = torch.sum((z-c) ** 2, dim=1)\n",
        "          losses = torch.where(sy == 0, dist, eta * ((dist + eps) ** sy.float()))\n",
        "          loss = torch.mean(losses)\n",
        "          score=dist\n",
        "          scores.append(score.detach().cpu())\n",
        "          labels.append(y.cpu())\n",
        "          epoch_loss+=loss\n",
        "          nb+=1\n",
        "  labels, scores = torch.cat(labels).numpy(), torch.cat(scores).numpy()\n",
        "  roc.append(roc_auc_score(labels, scores)*100)\n",
        "  mean.append(epoch_loss/nb)\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 170.151\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 135.092\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 109.937\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 92.076\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 79.379\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 70.294\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 63.669\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 58.564\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 54.459\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 50.975\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 47.880\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 45.067\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 42.587\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 40.298\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 38.245\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 36.288\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 34.621\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 33.142\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 31.751\n",
            "6647/6647: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 30.549\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 0, Loss: 0.762\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 1, Loss: 0.236\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 2, Loss: 0.138\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 3, Loss: 0.105\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 4, Loss: 0.087\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 5, Loss: 0.079\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 6, Loss: 0.070\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 7, Loss: 0.064\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 8, Loss: 0.062\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 9, Loss: 0.058\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 10, Loss: 0.054\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 11, Loss: 0.051\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 12, Loss: 0.050\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 13, Loss: 0.050\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 14, Loss: 0.048\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 15, Loss: 0.047\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 16, Loss: 0.044\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 17, Loss: 0.043\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 18, Loss: 0.043\n",
            "6647/6647: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 19, Loss: 0.042\n",
            "test for calss   0 .....\n",
            "Testing...\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 150.410\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 108.742\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 82.288\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 65.295\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 54.266\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 46.868\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 41.708\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 37.908\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 34.979\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 32.668\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 30.786\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 29.225\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 27.932\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 26.842\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 25.913\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 25.090\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 24.348\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 23.684\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 23.092\n",
            "7566/7566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 22.551\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 0, Loss: 0.311\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 1, Loss: 0.118\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 2, Loss: 0.088\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 3, Loss: 0.076\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 4, Loss: 0.069\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 5, Loss: 0.064\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 6, Loss: 0.061\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 7, Loss: 0.058\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 8, Loss: 0.056\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 9, Loss: 0.055\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 10, Loss: 0.052\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 11, Loss: 0.051\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 12, Loss: 0.050\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 13, Loss: 0.049\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 14, Loss: 0.048\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 15, Loss: 0.047\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 16, Loss: 0.046\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 17, Loss: 0.045\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 18, Loss: 0.044\n",
            "7566/7566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 19, Loss: 0.044\n",
            "test for calss   1 .....\n",
            "Testing...\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 154.306\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 121.914\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 98.230\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 82.573\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 71.870\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 64.729\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 59.706\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 55.914\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 52.897\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 50.465\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 48.463\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 46.784\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 45.409\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 44.209\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 43.183\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 42.267\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 41.470\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 40.759\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 40.106\n",
            "6686/6686: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 39.488\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 0, Loss: 0.405\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 1, Loss: 0.206\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 2, Loss: 0.165\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 3, Loss: 0.141\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 4, Loss: 0.128\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 5, Loss: 0.116\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 6, Loss: 0.105\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 7, Loss: 0.098\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 8, Loss: 0.092\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 9, Loss: 0.087\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 10, Loss: 0.083\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 11, Loss: 0.079\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 12, Loss: 0.075\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 13, Loss: 0.074\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 14, Loss: 0.070\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 15, Loss: 0.066\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 16, Loss: 0.066\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 17, Loss: 0.063\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 18, Loss: 0.062\n",
            "6686/6686: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 19, Loss: 0.060\n",
            "test for calss   2 .....\n",
            "Testing...\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 161.045\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 124.661\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 99.506\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 83.397\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 72.732\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 65.353\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 60.016\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 56.008\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 52.906\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 50.196\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 47.803\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 45.579\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 43.455\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 41.415\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 39.440\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 37.618\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 35.890\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 34.317\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 32.855\n",
            "6880/6880: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 31.494\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 0, Loss: 0.623\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 1, Loss: 0.252\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 2, Loss: 0.157\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 3, Loss: 0.123\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 4, Loss: 0.107\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 5, Loss: 0.100\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 6, Loss: 0.092\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 7, Loss: 0.085\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 8, Loss: 0.080\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 9, Loss: 0.076\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 10, Loss: 0.073\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 11, Loss: 0.071\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 12, Loss: 0.067\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 13, Loss: 0.066\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 14, Loss: 0.063\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 15, Loss: 0.062\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 16, Loss: 0.060\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 17, Loss: 0.059\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 18, Loss: 0.057\n",
            "6880/6880: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 19, Loss: 0.056\n",
            "test for calss   3 .....\n",
            "Testing...\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 152.706\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 116.467\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 91.660\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 76.324\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 66.866\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 60.470\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 55.674\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 51.836\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 48.706\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 46.191\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 44.189\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 42.583\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 41.236\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 40.074\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 39.054\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 38.169\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 37.398\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 36.699\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 36.069\n",
            "6556/6556: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 35.511\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 0, Loss: 0.257\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 1, Loss: 0.161\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 2, Loss: 0.127\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 3, Loss: 0.108\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 4, Loss: 0.097\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 5, Loss: 0.087\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 6, Loss: 0.079\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 7, Loss: 0.074\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 8, Loss: 0.069\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 9, Loss: 0.066\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 10, Loss: 0.063\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 11, Loss: 0.059\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 12, Loss: 0.057\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 13, Loss: 0.055\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 14, Loss: 0.053\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 15, Loss: 0.051\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 16, Loss: 0.050\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 17, Loss: 0.049\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 18, Loss: 0.048\n",
            "6556/6556: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 19, Loss: 0.047\n",
            "test for calss   4 .....\n",
            "Testing...\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 156.165\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 124.659\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 100.805\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 84.788\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 74.072\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 66.479\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 60.870\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 56.561\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 53.169\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 50.451\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 48.141\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 46.183\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 44.444\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 42.888\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 41.580\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 40.410\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 39.471\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 38.602\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 37.835\n",
            "6083/6083: [==============================>.] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 37.171\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 0, Loss: 0.682\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 1, Loss: 0.274\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 2, Loss: 0.178\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 3, Loss: 0.140\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 4, Loss: 0.122\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 5, Loss: 0.107\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 6, Loss: 0.098\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 7, Loss: 0.091\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 8, Loss: 0.086\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 9, Loss: 0.081\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 10, Loss: 0.078\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 11, Loss: 0.074\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 12, Loss: 0.074\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 13, Loss: 0.069\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 14, Loss: 0.067\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 15, Loss: 0.065\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 16, Loss: 0.063\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 17, Loss: 0.062\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 18, Loss: 0.060\n",
            "6083/6083: [==============================>.] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 19, Loss: 0.059\n",
            "test for calss   5 .....\n",
            "Testing...\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 168.043\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 132.515\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 107.246\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 88.197\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 74.868\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 65.745\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 59.141\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 54.080\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 50.057\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 46.652\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 43.887\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 41.538\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 39.482\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 37.724\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 36.086\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 34.652\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 33.314\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 31.953\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 30.765\n",
            "6641/6641: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 29.789\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 0, Loss: 0.472\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 1, Loss: 0.185\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 2, Loss: 0.113\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 3, Loss: 0.087\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 4, Loss: 0.074\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 5, Loss: 0.066\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 6, Loss: 0.060\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 7, Loss: 0.055\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 8, Loss: 0.052\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 9, Loss: 0.052\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 10, Loss: 0.048\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 11, Loss: 0.046\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 12, Loss: 0.045\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 13, Loss: 0.044\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 14, Loss: 0.041\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 15, Loss: 0.040\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 16, Loss: 0.039\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 17, Loss: 0.039\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 18, Loss: 0.038\n",
            "6641/6641: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 19, Loss: 0.038\n",
            "test for calss   6 .....\n",
            "Testing...\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 151.517\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 112.375\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 87.455\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 71.855\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 61.744\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 54.964\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 50.126\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 46.540\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 43.732\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 41.486\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 39.643\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 38.146\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 36.860\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 35.839\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 34.946\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 34.101\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 33.355\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 32.782\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 32.186\n",
            "7031/7031: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 31.637\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 0, Loss: 0.561\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 1, Loss: 0.212\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 2, Loss: 0.136\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 3, Loss: 0.112\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 4, Loss: 0.101\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 5, Loss: 0.100\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 6, Loss: 0.091\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 7, Loss: 0.082\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 8, Loss: 0.080\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 9, Loss: 0.080\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 10, Loss: 0.073\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 11, Loss: 0.071\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 12, Loss: 0.067\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 13, Loss: 0.069\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 14, Loss: 0.065\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 15, Loss: 0.062\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 16, Loss: 0.061\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 17, Loss: 0.060\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 18, Loss: 0.059\n",
            "7031/7031: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 19, Loss: 0.057\n",
            "test for calss   7 .....\n",
            "Testing...\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 163.031\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 127.614\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 103.174\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 86.373\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 75.258\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 67.882\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 62.700\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 58.783\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 55.592\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 53.057\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 50.981\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 49.233\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 47.672\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 46.248\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 44.943\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 43.748\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 42.653\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 41.632\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 40.694\n",
            "6566/6566: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 39.796\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 0, Loss: 0.494\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 1, Loss: 0.274\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 2, Loss: 0.182\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 3, Loss: 0.138\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 4, Loss: 0.114\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 5, Loss: 0.099\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 6, Loss: 0.090\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 7, Loss: 0.082\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 8, Loss: 0.077\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 9, Loss: 0.072\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 10, Loss: 0.069\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 11, Loss: 0.066\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 12, Loss: 0.064\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 13, Loss: 0.063\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 14, Loss: 0.061\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 15, Loss: 0.059\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 16, Loss: 0.057\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 17, Loss: 0.056\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 18, Loss: 0.055\n",
            "6566/6566: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 19, Loss: 0.054\n",
            "test for calss   8 .....\n",
            "Testing...\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 0, Loss: 159.039\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 1, Loss: 122.712\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 2, Loss: 96.724\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 3, Loss: 80.220\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 4, Loss: 69.621\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 5, Loss: 62.520\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 6, Loss: 57.331\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 7, Loss: 53.289\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 8, Loss: 49.990\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 9, Loss: 47.283\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 10, Loss: 45.077\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 11, Loss: 43.188\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 12, Loss: 41.556\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 13, Loss: 40.189\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 14, Loss: 39.033\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 15, Loss: 38.072\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 16, Loss: 37.208\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 17, Loss: 36.474\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 18, Loss: 35.788\n",
            "6676/6676: [===============================>] - ETA 0.2s\n",
            "Pretraining Autoencoder... Epoch: 19, Loss: 35.172\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 0, Loss: 0.671\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 1, Loss: 0.280\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 2, Loss: 0.157\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 3, Loss: 0.116\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 4, Loss: 0.099\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 5, Loss: 0.087\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 6, Loss: 0.078\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 7, Loss: 0.073\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 8, Loss: 0.069\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 9, Loss: 0.065\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 10, Loss: 0.062\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 11, Loss: 0.060\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 12, Loss: 0.057\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 13, Loss: 0.055\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 14, Loss: 0.053\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 15, Loss: 0.051\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 16, Loss: 0.050\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 17, Loss: 0.049\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 18, Loss: 0.048\n",
            "6676/6676: [===============================>] - ETA 0.1s\n",
            "Training Deep SAD... Epoch: 19, Loss: 0.048\n",
            "test for calss   9 .....\n",
            "Testing...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OduNXNF5t_a8",
        "outputId": "01a1b03b-bada-4e67-9111-bad178841ffa"
      },
      "source": [
        "print('#==============#==============#==============#==============#')\n",
        "print('|     CLASS    |   AUC SCORE  |   MEAN LOSS  |  ROC SCORE   |')\n",
        "for i in range(10):\n",
        "  print('|--------------|--------------|--------------|--------------|')\n",
        "  print('|    ',i,'       |    {:.2f}'.format(roc[i]),'    |      {:.2f}'.format(mean[i]),'   |     {:.2f}    |'.format(nscores[i]))\n",
        "print('#==============#==============#==============#==============#')"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#==============#==============#==============#==============#\n",
            "|     CLASS    |   AUC SCORE  |   MEAN LOSS  |  ROC SCORE   |\n",
            "|--------------|--------------|--------------|--------------|\n",
            "|     0        |    95.56     |      0.12    |     98.90    |\n",
            "|--------------|--------------|--------------|--------------|\n",
            "|     1        |    98.44     |      0.14    |     95.56    |\n",
            "|--------------|--------------|--------------|--------------|\n",
            "|     2        |    87.37     |      0.12    |     98.44    |\n",
            "|--------------|--------------|--------------|--------------|\n",
            "|     3        |    94.80     |      0.14    |     87.37    |\n",
            "|--------------|--------------|--------------|--------------|\n",
            "|     4        |    91.76     |      0.12    |     94.80    |\n",
            "|--------------|--------------|--------------|--------------|\n",
            "|     5        |    83.35     |      0.09    |     91.76    |\n",
            "|--------------|--------------|--------------|--------------|\n",
            "|     6        |    97.81     |      0.14    |     83.35    |\n",
            "|--------------|--------------|--------------|--------------|\n",
            "|     7        |    95.72     |      0.15    |     97.81    |\n",
            "|--------------|--------------|--------------|--------------|\n",
            "|     8        |    89.79     |      0.12    |     95.72    |\n",
            "|--------------|--------------|--------------|--------------|\n",
            "|     9        |    93.27     |      0.13    |     89.79    |\n",
            "#==============#==============#==============#==============#\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}